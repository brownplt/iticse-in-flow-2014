Reviews are potentially valuable for students, 
because they may help them improve their artifacts or their performance. 
The process of reviewing is also helpful for the reviewer, 
because they learn how to critically assess and how to provide constructive feedback. 
Producing high quality reviews is hard, 
and reviewers will benefit from feedback on their reviews. 
Thus, while students benefit from reviews, 
reviewers (who also may be students) benefit from meta-reviews.

According to Ramachandran and Gehringer[cite](rg:auto-assess-rev-lsa) reviews consist of 
(1) summative, (2) problem detection, and (3) advisory content, 
and meta-reviews can point out 
to which degree a review contains each of these three types of contents. 
Each of these types of review contents is valuable in its own way. 
While summative contents can reflect a reviewer's understanding, 
problem detection content directly helps a student to identify opportunities for improvement, 
and advisory content points out in which ways students might improve. 
Meta-reviews can include information on whether a review was helpful, 
or on which parts of a review were helpful. 
This information can be provided by the students receiving the review, 
for example by highlighting each section of a review they found helpful. 
This kind of information can help to train reviewers to not submit "brain dumps" 
of everything that might possibly be wrong, 
but to provide valuable but concise reviews. 
Moreover, students can be asked to link subsequent changes to their artifacts to specific points in reviews, 
similar to how software developers provide commit messages that link code changes to bug reports. 
Given appropriate tool support, 
the information also can be inferred automatically, 
based on whether or how an student changed their artifact after reading the review, 
or based on whether or not the quality of the artifact improved after the student read the review. 
Finally, meta-reviews also can take the form of rebuttals, 
and they can allow students to challenge the aspects of a review they disagree with.

While one generally may prefer to eliminate low quality contents in reviews, 
in a pedagogical context also receiving some low quality review contents can be beneficial. 
While in traditional educational settings students may trust all the feedback they receive from the instructor, 
in IFPR students have to learn to assess the value of the reviews they receive. 
They will have to learn to triage review comments into those they will act upon and those they will ignore. 
Moreover, having a diversity of reviews, 
maybe even contradicting reviews, 
can be a starting point for valuable discussions in class.

While summative feedback on review quality may not be very valuable for the reviewer, 
asking students to rate the reviews they receive on a Likert scale 
can provide the instructor with insight needed to moderate the overall learning process.

Meta-reviewing incurs a cost. 
Whether or not meta-reviews are worth that cost depends on the learning goals: 
if teaching how to review is important, meta-reviews are essential; 
however if the learning goals focus on artifact production or performance, 
and if the reviewers are experienced, 
meta-reviews may be less essential. 
An alternative to providing meta-reviews for each review is to provide a few example reviews and their meta-reviews. 
To not tempt students to simply reuse the best example review comments, 
these exemplar reviews can come from an assignment that is different from the current assignment.

Besides providing informative meta-reviews, 
[educators believing in the value of extrinsic motivation :-) ] might also wish to grade reviews. 
Such grades for reviews might simply be based on whether or not a review was submitted, 
or whether a review was submitted in a timely manner, 
or they may be based on any of the criteria mentioned above. 
Similarly, meta-reviews could be graded, too.

