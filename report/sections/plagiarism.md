The issues considered in this section are what can be done to prevent students gaming
the system, cheating, plagiarism and so forth. 
In the art courses, copying from masters is encouraged, as is knowing what has come before.  
Should you make someone know how to emulate a master before they go off on their own? 
Photocopying art is different from reproducing work of Picasso.
Consider civil engineering: you cannot  copy and paste a bridge. 
What’s different in our case?  
The perspective of assessing the quality of a code artefact brings up many of these plagiarism-related issues, 
because you have to assess the origin of the artefact.  
With IFPR, students get to see other students' solutions, opening the door to plagiarism.
In some sense this actually levels the playing field, because it reduces the advantage
of other students who already know how to copy things off the web.
Plagiarism has always been a back-channel issue, so opening up a new 
visible front-channel may not make much of a difference.
One possibility is that IFPR actually reduces the need to plagiarise, 
since the solutions are shown anyway when students are reviewing other students code.
Consequently, the real problem could be distinguishing copying from learning,
in particular, learning from feedback. Ultimately,
there is a question about how much of the final result is a
student's ``own work'', but in a 
collaborative learning model, that’s not so much of a concern --- the main
outcome is increased understanding.



The first way to mitigate this problem is using grading schemes,
for example,  by setting up the grading so that there is a lower weight on 
the final submission (say, 75\% on initial submission, 25\% on final submission).
This weighting also avoids problems of not submitting an adequate initial solution,
which would undermine the value of IFPR.
On the other hand, setting a low weight for the final vs initial solution means 
there is diminished value to learning from the feedback obtained on own work
and from reading (not copying) the work of others.
By structuring the assignment, such as by having a phase where tests are entered and
reviewed, this problem of diminished values of feedback can be mitigated to some degree.
A student cannot gain much from looking at someone else's test cases, but
their code would presumably better because of the test feedback.
Devising a grading scheme will likely always be problematic.
The selected weighting will likely reflect social norms about individual vs collective work,
and ultimately assessment can conflate/contradict the goals of IFPR.



Providing an oral examination is one (time-consuming) way of solving the problem.
This could consist of in-person code reviews which ask the student to explain their
code. This can be done by (1) asking them to understand some change to their submitted
code and (2) change their submitted code to match a new specification.
An oral exam such as this for each assignment would be a separate evaluation from the
artefacts that could be trusted regardless of plagiarism.
An additional means to mitigate the effects of plagiarism is to assess reviews: this offers
an alternative (or complementary) way of evaluating students' understanding rather than
evaluating just their artefacts.

One way of gaming the system --- a variant that requires a student to submit their reviews
before moving on to the next phase of the project --- is  to insert additional solutions
into the process. These instructor-provided  solutions may contain known bugs, and can be used to
control whether students are responding properly in their reviews, and
to make sure students can actually demonstrate knowledge from what they are given to review.
In the case that IFPR is being systematically gamed by students, the solution is 
to use it only on some problems.



A final way to mitigate plagiarism is to provide students with variations of a
problem, rather than giving them all the same problem. Such variations could possibly
be provided by or tailored to students.  A number of systems are available to help produce 
variants on an assignment  **Cite: Zeller’s system, Gulwani’s system.** 
A student might be more motivated to write one version (soccer game vs another kind of game), 
but can still be in a position to review another.
Using variations will produce different casts/perspectives on a problem, and this will help 
build the student's knowledge base.  Writing one variant and reviewing another forces the
student to generalize the point of the assignment across the two variants, even though
it is more burdensome to do so. This generalization has pedagogic value.
Nevertheless you need to make sure you know why you are creating different variants --- 
does it match your IFPR goals? Does it justify the costs?

A question that needs further exploration is whether variations makes IFPR reviewing harder,
because knowledge built-up in a student's solution no longer applies to what has to be reviewed.  
This could depend upon how different variants are and also on what the review rubric is.
Is there some meta-similarity that reflects what instructor really wants the students to learn from the assignment?
In order to ensure that the transferable knowledge is available for peer review,
higher-level rubrics may be required in order to discuss problems at the right level.

Another issue is that slight differences in problems may confound reviews, as students may
not be fully aware of the consequences of the differences, and may, in a sense, be 
viewing the assignment under review in terms of the specification of their own assignment.

As you dial up the delta between author work and reviewer work, how does review quality get affected?

If assignments are more individualized, rather than just variants, 
copying work from other individual assignments is not much of a concern, 
because the synthesis  is the contribution, not the individual code snippets used to produce the
assignment solution.


**Sentences Dave did not understand, or did not belong.**

Production of new content versus quality control/fitting pieces into an existing system.

On the other hand, reviewing different assignments in different phases produces more diversity in the reviews.


