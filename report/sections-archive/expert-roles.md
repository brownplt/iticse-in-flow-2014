Peer review can sometimes usefully be complemented with expert review.
However there are several arguments for and against combining the two.
Potential benefits of expert review include: having moderating roles
in the case of conflict; facilitating roles to encourage contribution;
overcoming cultural adjustment problems for students from cultures
where only expert opinions are given weight; improving the quality of
reviews received.  Alternate approaches which better encourage the
valuing of peer learning include: scaffolding the process of peer
review through suitable processes and rubrics; providing exemplars
(which both models good practice, and shifts the focus from the merits
of the person giving the review to the quality of the work itself);
meta-reviewing; and grading work based upon the reviews rather than
the original artefacts such as code.

Experts can act in the role of moderators to make sure that issues and
conflicts that arise in a live situation can be dealt with by an
authority figure.  In the role of moderators experts do not overtake
the role of students as peer reviewers, which supports the goal of
IFPR for students to naturally interact with one another directly.  So
moderation is a form of process rather than content expertise.
Another form of process expertise arises when experts are present not
for quality control reasons but as facilitators.  Students who are not
sure of themselves may not contribute much, and having experts not
involved means they may shrink further; experts can assist in getting
students to contribute - as persons who embody the idea of review as a
learning process (as opposed to/not just grading).

Expert reviewers can help address the cultural adjustment issues where
students devalue the opinions of their peers and overvalue the
opinions of instructors.  They can also offset deficiencies in the
knowledge and insight of students who are themselves adjusting to the
process of giving informed critiques, and ensure that some
knowledgeable, high quality feedback is received.  However if IFPR is
a desired practice to develop a culture of peer learning and support,
to encourage an atmosphere of insightful critique and constructive
feedback, then these goals for the course should be spelt out from the
outset and various forms of scaffolding and support should be put in
place to manage this adjustment process.

The argument for the necessity of expert feedback can be contrasted
with the findings of a study [CITE] that measured the quality of
improvement with multiple peers, single expert, and single peer, but
students didn't know whether reviews were from an expert or from a
peer.  The study concluded that the quality of feedback was [FILL].

Several questions in relation to the role of experts arise in the
context of IFPR.  How does knowing the reviewer affect perception of
quality of the review?  If there is some expert feedback, will
students ignore feedback from the non-experts?  Feedback from experts
may be interpreted as "more relevant for my grade", and hence more
likely to be acted upon?

In IFPR the goal is to avoid attaching the idea of constructive review
to an expert being present. However, an expert can provide models for
review that students can follow. So one strategy would have experts
give examples, or be present for some (early) sessions and not
others. These exemplars and supports represent a form of scaffolding
reviewing.


We can consider four different roles of experts: as non-judging
moderator, as facilitator, as evaluator, and expert as a
meta-reviewer.  In this latter role, and through providing exemplars,
the expert may perform a supportive role, yet consistent with the
tenets of IFPR, in scaffolding the learning process.

In discussion among the working group members, the idea of an
"Editor's picks" option was discussed, where good examples were
highlighted.  TAs might monitor reviews and post a handful of really
good ones for the entire class.  This was considered to be a potential
problem in terms of punishment/reward, because it externally rewards
good examples, and reinforces an extrinsic motivational model [CITE].
However, things that might not be selected for profiling would not
necessarily be bad, but just not picked.  Yet students do want
examples of how it should be done.  Meta reviews may offer a solution
to the problem of "giving rewards" through an approach such as 'editor
picks', or choosing solutions to display. In providing meta-reviews it
will be important to explain to students specifically why something is
considered good/bad.  The positive aspect in this approach is that
showing exemplars shifts the emphasis from quality of person
(reviewer)/authority of person to authority of the work.

In an extension of this practice instructors could ask students to
grade a set of reviews of an assignment, or to write
meta-reviews. This would form part of training students to identify
good vs bad reviews.  It would also help students align themselves
with quality work in the reviewing process. The comparative analysis
of reviews would also seem valuable here.

One member of the working group mentioned his experience in a course
setting, of asking students about the value of talking about design,
which the students deemed valuable. From this and other experiences
with their students, the working group members believe that students
value the learning benefits of the meta-cognitive and reflective
activity, which is involved in peer review.  In this process it is
important to consider reviewing's two roles from a student perspective
- demonstrating the student's own understanding as well as providing
feedback to help another student. One of the weaknesses of reviewing a
fine piece of work is that it is not clear to students how and what to
critique.  To mitigate "Looks Good to Me" reviews on good work, it is recommend
that emphasis be placed on the former. To support such more
constructive reviews a rubric could be adopted that talks positively
about strengths and explanations as well as problems.

Cho and MacArthur compare three approaches to giving feedback on
written assignments in a psychology course: feedback from a single
peer, feedback from a single topic expert, and feedback from multiple
peers [cite](cho-peer-expert-reviewing10). The results indicate that
feedback from multiple peers results in better quality revisions than
feedback from an expert, with feedback from a single peer being the
worse.  The hypothesised reason for this was that peers gave feedback
that was phrased in terms that students could more easily comprehend.
It certainly would be interesting to see whether the same results
applied to programming assignments.

