Assignment: Search about Expertiza, and find out what they do for
review-of-review, and how they to automatic evaluation of reviews.  Some links
to get started are below, but you don't have to read them all, we're
specifically looking for how this project handles feedback on reviews and
evaluation of reviews

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.8397
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.416.228
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5992285&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5992285


Fill in a summary for a related work section, and for your fellow WG members:

Expertiza, which grew out of an approach initiated in 1986, does not just include the idea of peer-review of peer-reviews, 
but also the goal of students creating learning objects (examples, explanations, and exercises).

Expertiza peer reviews are based on a rubric with numeric scores and further comments. 
Gehringer et al. [1] quote Van Lehn et al. [VL95] who say that students who assess their peers' work 
benefit by (1) reviewing, (2) summarizing, (3) diagnosing misconceived knowledge, and 
(4) considering deviations from the ideal. 
Lakshmi and Gehringer [2] quote Rada et al. [R94], who 
"found that students who evaluated their peersâ€™ work were more likely to improve the quality of their own work 
than those students who did not provide peer reviews."

Lakshmi and Gehringer [2] state that a review should 
"help the authors identify mistakes" and "help them learn possible ways of fixing them". 
They call the process of evaluating reviews "meta reviewing". 
They claim that manual meta reviewing "is (a) slow, (b) prone to errors, and (c) likely to be prone to errors". 
To provide consistent, bias-free meta reviews, 
they develop an automated meta reviewing tool, 
using the following metrics for how to assess reviews: 
"review relevance", "content type" (summative, problem detection, advisory), 
"tone" (positive, negative, neutral), "quantity", and "plagiarism". 
They evaluated their tool with a user experience study involving 24 participants and 
found "relevance" to be the most important and "quantity" to be the least important metric.

Personal notes:
I don't like Gehringer et al.'s [1] "spin" of this reducing the drudgery for instructors and TAs. 
I think it should focus on the improvement of learning instead! 
The idea of having students create problems (instead of solving them) 
is great and should stand on its own (without this "administrative" side benefit of reducing instructor load). 

Lakshmi and Gehringer [2] define reviews as "text-based feedback". 
I don't see why they would HAVE to be text based. 
It might be that other forms of representations (e.g., annotations, diagrams) 
might be more effective than simple text.

Interestingly, using Expertiza for code-review lead to mixed results [1].

[2] provides discussion of related work including some papers I still need to look at.

I haven't read [3] yet.

